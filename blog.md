ğŸš¨ Responsible AI Detective Files: Case 001 & 002
Case 1: The Hiring Bot That Hates Career Gaps ğŸ‘©â€ğŸ’¼ğŸ¤–

Whatâ€™s happening?
A companyâ€™s AI is filtering job applications. On paper, it looks like a productivity hero â€” zooming through thousands of CVs faster than HR could dream.

Whatâ€™s fishy?
The AI quietly rejects more women with career gaps, often caused by maternity leave or caregiving. Thatâ€™s bias hiding in plain sight. Not fair, not cool.

How to fix it:
Retrain the model with fairness constraints and audit the data so it doesnâ€™t punish career breaks. Also, add a human-in-the-loop to review edge cases.

Case 2: The School Proctoring AI That Watches Too Hard ğŸ“ğŸ‘€

Whatâ€™s happening?
During online exams, an AI watches studentsâ€™ eyes like a hawk ğŸ¦…. If you look away from the screen too much, it screams: â€œCHEATER!â€

Whatâ€™s fishy?
Neurodivergent students or those with certain disabilities get flagged unfairly. Thatâ€™s a fairness and inclusivity problem, plus it can stress students out â€” not exactly the vibe for learning.

How to fix it:
Make the system more transparent: show students how theyâ€™re being monitored, and let teachers review AI flags before punishing. Even better, use multi-modal checks (like keystroke or audio patterns) instead of relying only on eye movements.

ğŸ“ Detectiveâ€™s Closing Note

AI is powerful, but without fairness, transparency, and accountability, it becomes more of a villain than a sidekick. Like any good detective story, the key is to look for the hidden clues â€” the biases, the blind spots, the missing explanations â€” and fix them so AI serves everyone.
