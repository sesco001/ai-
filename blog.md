🚨 Responsible AI Detective Files: Case 001 & 002
Case 1: The Hiring Bot That Hates Career Gaps 👩‍💼🤖

What’s happening?
A company’s AI is filtering job applications. On paper, it looks like a productivity hero — zooming through thousands of CVs faster than HR could dream.

What’s fishy?
The AI quietly rejects more women with career gaps, often caused by maternity leave or caregiving. That’s bias hiding in plain sight. Not fair, not cool.

How to fix it:
Retrain the model with fairness constraints and audit the data so it doesn’t punish career breaks. Also, add a human-in-the-loop to review edge cases.

Case 2: The School Proctoring AI That Watches Too Hard 🎓👀

What’s happening?
During online exams, an AI watches students’ eyes like a hawk 🦅. If you look away from the screen too much, it screams: “CHEATER!”

What’s fishy?
Neurodivergent students or those with certain disabilities get flagged unfairly. That’s a fairness and inclusivity problem, plus it can stress students out — not exactly the vibe for learning.

How to fix it:
Make the system more transparent: show students how they’re being monitored, and let teachers review AI flags before punishing. Even better, use multi-modal checks (like keystroke or audio patterns) instead of relying only on eye movements.

📝 Detective’s Closing Note

AI is powerful, but without fairness, transparency, and accountability, it becomes more of a villain than a sidekick. Like any good detective story, the key is to look for the hidden clues — the biases, the blind spots, the missing explanations — and fix them so AI serves everyone.
